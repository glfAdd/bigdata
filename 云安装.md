[官网文档](http://hadoop.apache.org/docs/)

[低版本中文文档](http://hadoop.apache.org/docs/r1.0.4/cn/cluster_setup.html)

[坑](https://www.kancloud.cn/bizzbee/bigdata/1316702)

##### 集群状态查看

```
========= centos01 =======
7632 NodeManager
8338 Bootstrap
6935 NameNode
7964 RunJar
7087 DataNode
7487 ApplicationHistoryServer
8127 RunJar
========= centos02 =======
5266 NodeManager
4876 DataNode
5119 ResourceManager
========= centos03 =======
5762 SecondaryNameNode
5541 DataNode
5644 JobHistoryServer
5916 NodeManager
```

##### centos 需要的软件

```bash
$ yum install -y epel-release
$ yum clean all
$ yum makecache
$ yum update
$ yum install -y vim net-tools passwd sudo git zsh htop tree telnet telnet-server iotop
```

##### 安装 ssh 

```
yum install -y openssl openssh-server openssh-clients
```

##### 设置免密登录

```bash
# 生成密钥对
$ ssh-keygen -t rsa 

# 方式1: 创建 authorized_keys 文件, 添加公钥

# 方式2: 会将本机的公钥发送到要访问计算机, 存储在 ~/.ssh/authorized_keys
$ ssh-copy-id centos01

# 如果仍然无法登录需要修改该文件权限
chmod 644 ~/.ssh/authorized_keys
chmod 700 ~/.ssh
```

##### 修改 hosts 文件

```
172.20.0.101 centos01
172.20.0.102 centos02
172.20.0.103 centos03
```

##### 安装 jdk 和 hadoop

- download

  ```bash
  $ wget hadoop https://mirrors.bfsu.edu.cn/apache/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar.gz
  ```

- 设置环境变量, 编辑 /etc/profile.d/my_env.sh 文件

  ```shell
  #JAVA_HOME
  export JAVA_HOME=/opt/jdk1.8.0_321
  export PATH=$PATH:$JAVA_HOME/bin
  
  #HADOOP_HOME
  export HADOOP_HOME=/opt/hadoop-3.3.1
  export PATH=$PATH:$HADOOP_HOME/bin
  export PATH=$PATH:$HADOOP_HOME/sbin
  ```

- 使配置生效

  ```bash
  $ source /etc/profile
  ```

- 验证

  ```bash
  $ java -version
  $ hadoop version
  ```

##### 创建 user

```bash
$ adduser gong
```

##### xsync 集群分发脚本

- 创建脚本 xsync.sh

```shell
#!/bin/bash
#1. 判断参数个数
if [ $# -lt 1 ]
then
	echo Not Enough Arguement!
	exit;
fi
#2. 遍历集群所有机器
for host in centos01 centos02 centos03
do
	echo ====================  $host  ====================
	#3. 遍历所有目录，挨个发送
	for file in $@
	do
		#4 判断文件是否存在
		if [ -e $file ]
		then
			#5. 获取父目录
			pdir=$(cd -P $(dirname $file); pwd)
			#6. 获取当前文件的名称
			fname=$(basename $file)
			ssh $host "mkdir -p $pdir"
			rsync -av $pdir/$fname $host:$pdir
		else
			echo $file does not exists!
		fi
	done
done
```

- 使用

```bash
# 修改权限
chmod +x xsync

# 将脚本移动到/bin中，以便全局调用
sudo mv xsync /bin/

# 测试脚本
sudo xsync /bin/xsync

# 使用
xsync /opt/a.txt
```

##### 查看集群 jps 脚本

```shell
#!/bin/bash
for i in centos01 centos02 centos03; do
	echo "========= $i =========="
	ssh $i "jps" | grep -v Jps
done
```

##### 修改计算机 hosts 文件

> 所在机器的 ip 使用内网 ip
>
> C:\Windows\System32\drivers\etc\hosts

```
1.1.1.1 centos01
1.1.1.1 centos02
1.1.1.1 centos03
```

## 配置 hadoop

##### 2.x 和 3.x 端口变化

| 节点 | 2.x   | 3.x  | name                                 | desc                                                         |
| ---- | ----- | ---- | ------------------------------------ | ------------------------------------------------------------ |
| nn   | 50470 | 9871 | dfs.namenode.https-address           | The namenode secure http server address and port.            |
| nn   | 50070 | 9870 | dfs.namenode.http-address            | The address and the base port where the dfs namenode web ui will listen on. |
| nn   | 8020  | 9820 | fs.defaultFS                         | 指定 HDFS 运行时 nameNode 地址                               |
| 2nn  | 50091 | 9869 | dfs.namenode.secondary.https-address | The secondary namenode HTTPS server address and port         |
| 2nn  | 50090 | 9868 | dfs.namenode.secondary.http-address  | The secondary namenode HTTPS server address and port         |
| dn   | 50020 | 9867 | dfs.datanode.ipc.address             | The datanode ipc server address and port.                    |
| dn   | 50010 | 9866 | dfs.datanode.address                 | The datanode server address and port for data transfer.      |
| dn   | 50475 | 9865 | dfs.datanode.https.address           | The datanode secure http server address and port             |
| dn   | 50075 | 9864 | dfs.datanode.http.address            | The datanode http server address and por                     |
| yarn | /     | 8088 | yarn.resourcemanager.webapp.address  | http 服务端口                                                |

##### hadoop目录

| 目录  | 说明                                            |
| ----- | ----------------------------------------------- |
| bin   | 存放对Hadoop相关服务（HDFS,YARN）进行操作的脚本 |
| sbin  | 存放启动或停止Hadoop相关服务的脚本              |
| etc   | Hadoop的配置文件目录，存放Hadoop的配置文件      |
| lib   | 存放Hadoop的本地库（对数据进行压缩解压缩功能）  |
| share | 存放Hadoop的依赖jar包、文档、和官方案例         |

##### 集群分配

```
1. NameNode 和 SecondaryNameNode不要安装在同一台服务器
2. ResourceManager 也很消耗内存，不要和NameNode、SecondaryNameNode配置在同一台机器上。
```

##### core-site.xml

````xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <!-- 指定 NameNode 的地址 因为是集群地址，集群都指向这个地址节点 -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://centos01:9820</value>
    </property>
    <!-- 指定 hadoop 数据的存储目录 -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/opt/hdpdata</value>
    </property>
    <!-- 配置HDFS网页登录使用的静态用户为gong -->
    <property>
        <name>hadoop.http.staticuser.user</name>
        <value>gong</value>
    </property>
    <!-- 配置该gong(superUser)允许通过代理访问的主机节点 -->
    <property>
        <name>hadoop.proxyuser.gong.hosts</name>
        <value>*</value>
    </property>
    <!-- 配置该gong(superUser)允许通过代理用户所属组 -->
    <property>
        <name>hadoop.proxyuser.gong.groups</name>
        <value>*</value>
    </property>
    <!-- 配置该gong(superUser)允许通过代理的用户-->
    <property>
        <name>hadoop.proxyuser.gong.groups</name>
        <value>*</value>
    </property>
</configuration>

````

##### hdfs-site.xml

 ```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <!-- nn web端访问地址-->
    <property>
        <name>dfs.namenode.http-address</name>
        <value>centos01:9870</value>
    </property>
    <!-- 2nn web端访问地址-->
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>centos03:9868</value>
    </property>
</configuration>

 ```

##### yarn-site.xml

```xml
<configuration>
    <!-- 指定MR走shuffle -->
    <!-- nomenodeManager 获取数据的方式是 shuffle -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <!-- 指定 ResourceManager 地址-->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>centos02</value>
    </property>
    <!-- 环境变量的继承 -->
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
    <!-- 开启日志聚集功能 -->
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
    </property>
    <!-- 日志服务器地址 -->
    <property>
        <name>yarn.log.server.url</name>
        <value>http://centos01/jobhistory/logs</value>
    </property>
    <!-- 日志保存时间 -->
    <property>
        <name>yarn.log-aggregation.retain-seconds</name>
        <value>604800</value>
    </property>
    <property>
        <name>yarn.resourcemanager.system-metrics-publisher.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.timeline-service.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.timeline-service.hostname</name>
        <value>centos01</value>
    </property>
    <property>
        <name>yarn.timeline-service.http-cross-origin.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.timeline-service.generic-application-history.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.timeline-service.address</name>
        <value>centos01:10201</value>
    </property>
    <property>
        <name>yarn.timeline-service.webapp.address</name>
        <value>centos01:8188</value>
    </property>
    <property>
        <name>yarn.timeline-service.webapp.https.address</name>
        <value>centos01:2191</value>
    </property>
    <property>
        <name>yarn.timeline-service.handler-thread-count</name>
        <value>24</value>
    </property>
</configuration>

```

##### mapres-site.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <!-- 指定 MapReduce 程序运行在 Yarn 上 -->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <!-- 历史服务器端地址 -->
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>centos03:10020</value>
    </property>
    <!-- 历史服务器 web 端地址 -->
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>centos03:19888</value>
    </property>
</configuration>

```

##### workers

- ./etc/hadoop/workers 添加如下内容, 文件中不允许有空行

```xml
centos01
centos02
centos03
```

##### 集群时间同步

```

```

##### 集群启动方式 1

> 需要配置好ssh

- 如果集群是第一次启动需要先格式化 NameNode (使用centos01 NN)
- 格式化之前，一定要先停止上次启动的所有namenode和datanode进程，然后再删除data和log数据

```bash
./sbin/stop-all.sh

# 1. 格式化 (centos01配置了namenode的节点上运行)
$ hdfs namenode -format

# 2. 启动HDFS (centos01配置了namenode的节点上运行)
$ ./sbin/start-dfs.sh

# 3. 在配置了ResourceManager的节点（centos02）启动YARN
$ ./sbin/start-yarn.sh

# 5. 启动历史服务器 (centos03)
$ mapred --daemon start historyserver

# 6. 启动 Timelineserver (centos01)
$ yarn --daemon start timelineserver
```

##### 集群启动方式 2

```bash
# hdfs 组件
$ hdfs --daemon start/stop namenode/datanode/secondarynamenode
# yarn 组件
$ yarn --daemon start/stop  resourcemanager/nodemanager/timelineserver

$ mapred --daemon start historyserver
```

##### web

```
hdfs的web管理页面
http://centos01:9870


yarn的web管理界面
http://centos02:8088/cluster


Web端查看SecondaryNameNode(不好用)
http://centos03:9868/status.html


日志服务器地址
http://centos03:19888/jobhistory
```

##### 使用命令

```
[gong@centos01 ~]$ hdfs fsck /
Connecting to namenode via http://centos01:9870/fsck?ugi=gong&path=%2F
FSCK started by gong (auth:SIMPLE) from /172.20.0.101 for path / at Sun May 30 02:45:08 UTC 2021


Status: HEALTHY //集群状态健康
 Number of data-nodes:	3 //集群有多少数据节点
 Number of racks:		1 //集群有多少机架
 Total dirs:			1 //集群有多少个目录
 Total symlinks:		0 //集群有多少链接

Replicated Blocks:
 Total size:	0 B //集群总共文件大小
 Total files:	0 //集群有多少个文件
 Total blocks (validated):	0  //多少个数据块  通过验证的有2块  平均块大小是27xxx
 Minimally replicated blocks:	0 //最少复制的块数
 Over-replicated blocks:	0 //超过预计复制数目的块有几个（比如说 多备份了1个副本）
 Under-replicated blocks:	0 // 少于预计复制数目的块有几个（比如说 3副本少备份了1个副本）
 Mis-replicated blocks:		0 //没备份的块有几个
 Default replication factor:	3 //备份因子为1  每个数据块给1个备份
 Average block replication:	0.0  //平均备份数目，1个备份
 Missing blocks:		0
 Corrupt blocks:		0
 Missing replicas:		0

Erasure Coded Block Groups:
 Total size:	0 B
 Total files:	0
 Total block groups (validated):	0
 Minimally erasure-coded block groups:	0
 Over-erasure-coded block groups:	0
 Under-erasure-coded block groups:	0
 Unsatisfactory placement block groups:	0
 Average block group size:	0.0
 Missing block groups:		0
 Corrupt block groups:		0
 Missing internal blocks:	0
FSCK ended at Sun May 30 02:45:08 UTC 2021 in 9 milliseconds


The filesystem under path '/' is HEALTHY
```

## mysql (二进制安装)

> 官网: https://dev.mysql.com/downloads/mysql/

##### 创建用户

```
groupadd mysql
useradd mysql -g mysql -s /sbin/nologin
```

#####  下载

```
选择最小安装
$ wget https://dev.mysql.com/get/Downloads/MySQL-8.0/mysql-8.0.25-linux-glibc2.17-x86_64-minimal.tar.xz
$ tar -Jxvf mysql-8.0.25-linux-glibc2.17-x86_64-minimal.tar.xz
$ cd mysql-8.0.25-linux-glibc2.17-x86_64-minimal
```

##### 创建文件修改该权限

```bash
$ mkdir data
$ mkdir 3306
$ chmod 755 -R data 
$ chmod 755 -R 3306 
$ sudo chown -R mysql:mysql data
$ sudo chown -R mysql:mysql 3306 
```

##### 设置配置文件

- 创建文件

  ```bash
  $ mkdir config && cd config
  ```

- config 下创建 my.cnf 文件

  ```
  [client]
  port = 3306
  socket = /opt/mysql-8.0.25-linux-glibc2.17-x86_64-minimal/3306/mysql.sock
  default-character-set=utf8mb4
  
  [mysql]
  disable-auto-rehash #允许通过TAB键提示
  default-character-set = utf8mb4
  connect-timeout = 10
  socket = /opt/mysql-8.0.25-linux-glibc2.17-x86_64-minimal/3306/mysql.sock
  
  [mysqld]
  user=mysql
  server-id = 3306
  port = 3306
  socket = /opt/mysql-8.0.25-linux-glibc2.17-x86_64-minimal/3306/mysql.sock
  pid-file = /opt/mysql-8.0.25-linux-glibc2.17-x86_64-minimal/3306/mysql.pid
  basedir = /opt/mysql-8.0.25-linux-glibc2.17-x86_64-minimal
  datadir = /opt/mysql-8.0.25-linux-glibc2.17-x86_64-minimal/data
  #bind_address = 10.10.10.11
  autocommit = 1
  
  default_authentication_plugin=mysql_native_password
  character-set-server=utf8mb4
  explicit_defaults_for_timestamp=true
  lower_case_table_names=1
  back_log=103
  max_connections=10000
  max_connect_errors=100000
  table_open_cache=512
  external-locking=FALSE
  max_allowed_packet=32M
  sort_buffer_size=2M
  join_buffer_size=2M
  thread_cache_size=51
  transaction_isolation=READ-COMMITTED
  tmp_table_size=96M
  max_heap_table_size=96M
  
  
  #logs
  long_query_time = 10
  slow_query_log = 1
  slow_query_log_file=/opt/mysql-8.0.25-linux-glibc2.17-x86_64-minimal/3306/slow.log
  
  #log-warnings = 1
  log_error_verbosity=3
  
  log-error = /opt/mysql-8.0.25-linux-glibc2.17-x86_64-minimal/3306/mysql.err
  log_output = FILE #参数log_output指定了慢查询输出的格式，默认为FILE，你可以将它设为TABLE，然后就可以查询mysql架构下的slow_log表了
  
  
  #log-queries-not-using-indexes
  #log-slow-slave-statements
  max_binlog_size = 1G
  #max_relay_log_size = 1G
  
  # replication
  log-bin=/opt/mysql-8.0.25-linux-glibc2.17-x86_64-minimal/3306/mysql-bin
  #server-id=1
  #binlog_format= ROW
  #gtid_mode = on
  #enforce_gtid_consistency = 1
  #log_slave_updates   = 1
  #master-info-repository=TABLE
  #relay-log-info-repository=TABLE
  
  # innodb storage engine parameters
  innodb_buffer_pool_size=500M
  innodb_data_file_path=ibdata1:100M:autoextend:max:5G #redo
  innodb_temp_data_file_path = ibtemp1:100M:autoextend:max:10G
  #innodb_file_io_threads=4 #默认的是4
  innodb_log_buffer_size=16M
  innodb_log_file_size=256M #undo
  innodb_log_files_in_group=2
  innodb_lock_wait_timeout=50
  innodb_file_per_table=1 #独立表空间
  ```

##### 初始化

```
方式1: 使用配置文件, 文件中生成随机密码
$ bin/mysqld --defaults-file=config/my.cnf --user=mysql --initialize

密码在 3306/mysql.err 文件中
2021-06-21T07:58:23.251821Z 6 [Note] [MY-010454] [Server] A temporary password is generated for root@localhost: 2p<hp3:CHjXj


方式2: 参数 --initialize-insecure 设置空密码 (使用这个)
$ bin/mysqld --defaults-file=config/my.cnf --user=mysql --initialize-insecure 


方式3: 参数设置目录
$ bin/mysqld --user=mysql  --initialize --basedir=/opt/mysql-8.0.25-linux-glibc2.17-x86_64-minimal --datadir=/opt/mysql-8.0.25-linux-glibc2.17-x86_64-minimal/data
```

##### 启动服务

```bash
/opt/mysql-8.0.25-linux-glibc2.17-x86_64-minimal/bin/mysqld_safe --defaults-file=/opt/mysql-8.0.25-linux-glibc2.17-x86_64-minimal/config/my.cnf &
```

##### 首次登录修改密码

```
$ bin/mysql -uroot -S /opt/mysql-8.0.25-linux-glibc2.17-x86_64-minimal/3306/mysql.sock

# 修改密码
alter user 'root'@'localhost' identified by '123456';
```

##### 登录

```
$ bin/mysql -uroot -S /opt/mysql-8.0.25-linux-glibc2.17-x86_64-minimal/3306/mysql.sock -p
```

##### 允许外网访问

```
use mysql;
select host,user from user;
update user set host='%' where user='root';
flush privileges;
select host,user from user;
```

##### 使用中问题

- 问题: 初始化问题

  ```
  错误信息:
  ./mysqld: error while loading shared libraries: libaio.so.1: cannot open shared object file: No such file or directory
  
  解决办法:
  centos: yum install numactl
  ubuntu: aptitude install libaio1 libaio-dev
  ```
  
- 问题: mysql 启动问题

  ```
  错误信息:
  ./mysql: error while loading shared libraries: libncurses.so.5: cannot open shared object file: No such file or directory
  
  解决办法:
  ubuntu
  aptitude install libncurses5
  ```
  
- 问题: 客户端服务链接mysql

  ```
  错误信息:
  ERROR 1045 (28000): Access denied for user 'glfadd'@'localhost' (using password: NO)
  
  解决办法:
  $ bin/mysql -uroot -S /opt/mysql-8.0.25-linux-glibc2.17-x86_64-minimal/3306/mysql.sock
  ```

##### 生成软链接 (未使用)

```bash
$ cd /usr/local
$ ln -s /opt/mysql-8.0.20-linux-x86_64-minimal mysql
```

## Hive

> hive 元数据存储在 mysql 中, mysql 必须先运行

##### hive 安装

```bash
1. 下载解压到docker /opt目录下
$ wget https://mirrors.bfsu.edu.cn/apache/hive/hive-3.1.2/apache-hive-3.1.2-bin.tar.gz


2. 设置环境变量, 编辑 /etc/profile.d/my_env.sh 文件
export HIVE_HOME=/opt/apache-hive-3.1.2-bin
export PATH=$PATH:$HIVE_HOME/bin


3. 使配置生效
source /etc/profile


4. 解决日志Jar包冲突
$ mv $HIVE_HOME/lib/log4j-slf4j-impl-2.10.0.jar $HIVE_HOME/lib/log4j-slf4j-impl-2.10.0.jar.bak
```

##### hive 元数据配置到MySql

- 下载 MySQL 对应版本的 JDBC 

```bash
官网: https://dev.mysql.com/downloads/connector/j/
选择 "Platform Independent (Architecture Independent), Compressed TAR Archive"

$ wget https://cdn.mysql.com//Downloads/Connector-J/mysql-connector-java-8.0.25.tar.gz

将版本对应的JDBC驱动 mysql-connector-java-8.0.25.jar 拷贝到 $HIVE_HOME/lib 目录下
```

- 数据在hdfs中的存储位置

```bash
$ hdfs dfs -mkdir -p /usr/hive/warehouse
$ hdfs dfs -mkdir -p /usr/hive/tmp
$ hdfs dfs -mkdir -p /usr/hive/log
$ hdfs dfs -chmod g+w /usr/hive/warehouse
$ hdfs dfs -chmod g+w /usr/hive/tmp
$ hdfs dfs -chmod g+w /usr/hive/log
```

- 在 $HIVE_HOME/conf 目录下新建 hive-site.xml 文件

```xml
<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration> 
  <property> 
    <name>javax.jdo.option.ConnectionURL</name>  
    <value>jdbc:mysql://centos01:3306/metastore?useSSL=false</value> 
  </property>  
  <property> 
    <name>javax.jdo.option.ConnectionDriverName</name>  
    <value>com.mysql.jdbc.Driver</value> 
  </property>  
  <property> 
    <name>javax.jdo.option.ConnectionUserName</name>  
    <value>root</value> 
  </property>  
  <property> 
    <name>javax.jdo.option.ConnectionPassword</name>  
    <value>123456</value> 
  </property>  
  <property> 
    <name>hive.metastore.warehouse.dir</name>  
    <value>/usr/hive/warehouse</value> 
  </property>  
  <property> 
    <name>hive.metastore.schema.verification</name>  
    <value>false</value> 
  </property>  
  <property> 
    <name>hive.metastore.uris</name>  
    <value>thrift://centos01:9083</value> 
  </property>  
  <property> 
    <name>hive.server2.thrift.port</name>  
    <value>10000</value> 
  </property>  
  <property> 
    <name>hive.server2.thrift.bind.host</name>  
    <value>centos01</value> 
  </property>  
  <property> 
    <name>hive.metastore.event.db.notification.api.auth</name>  
    <value>false</value> 
  </property> 
</configuration>
```

## 安装 tez 引擎

> hive有三种引擎：mapreduce、spark、tez，默认引擎为MapReduce，但MapReduce的计算效率非常低，而Spark和Tez引擎效率高，公司一般会使用Spark或Tez作为hive的引擎。
>
> 官网: http://tez.apache.org/
> 下载地址: https://mirrors.bfsu.edu.cn/apache/tez/

##### 下载

```bash
1. 下载解压到 centos01
$ wget https://mirrors.bfsu.edu.cn/apache/tez/0.10.0/apache-tez-0.10.0-bin.tar.gz
$ tar zxvf apache-tez-0.10.0-bin.tar.gz

2. 上传到 hdfs 中
$ hadoop fs -mkdir /tez
$ hadoop fs -put /opt/apache-tez-0.10.0-bin.tar.gz /tez
```

##### tez-site.xml

> 新建 $HADOOP_HOME/etc/hadoop/tez-site.xml 文件

```xml
<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration> 
  <!--指明hdfs集群上的tez的tar包，使Hadoop可以自动分布式缓存该jar包-->
  <property> 
    <name>tez.lib.uris</name>  
    <value>${fs.defaultFS}/tez/apache-tez-0.10.0-bin.tar.gz</value> 
  </property>  
  <property> 
    <name>tez.use.cluster.hadoop-libs</name>  
    <value>true</value> 
  </property>  
  <property> 
    <name>tez.am.resource.memory.mb</name>  
    <value>1024</value> 
  </property>  
  <property> 
    <name>tez.am.resource.cpu.vcores</name>  
    <value>1</value> 
  </property>  
  <property> 
    <name>tez.container.max.java.heap.fraction</name>  
    <value>0.4</value> 
  </property>  
  <property> 
    <name>tez.task.resource.memory.mb</name>  
    <value>1024</value> 
  </property>  
  <property> 
    <name>tez.task.resource.cpu.vcores</name>  
    <value>1</value> 
  </property> 
</configuration>
```

##### tez.sh

> 新建 $HADOOP_HOME/etc/hadoop/shellprofile.d/tez.sh 添加Tez的Jar包相关信息

```
hadoop_add_profile tez
function _tez_hadoop_classpath
{
    hadoop_add_classpath "$HADOOP_HOME/etc/hadoop" after
    hadoop_add_classpath "/opt/apache-tez-0.10.0-bin/*" after
    hadoop_add_classpath "/opt/apache-tez-0.10.0-bin/lib/*" after
}
```

##### hive-site.xml

> 修改Hive的计算引擎, 编辑 $HIVE_HOME/conf/hive-site.xml 添加

```xml
<property>
    <name>hive.execution.engine</name>
    <value>tez</value>
</property>
<property>
    <name>hive.tez.container.size</name>
    <value>1024</value>
</property>
```

##### 解决日志Jar包冲突

```
$ mv /opt/apache-tez-0.10.0-bin/lib/slf4j-log4j12-1.7.10.jar /opt/apache-tez-0.10.0-bin/lib/slf4j-log4j12-1.7.10.jar.bak
```

##### 初始化元数据库

```bash
1. 登陆MySQL


2. 新建Hive元数据库
mysql> create database metastore;
mysql> quit;


3. 初始化Hive元数据库
$ schematool -initSchema -dbType mysql -verbose
```

##### 设置 hive log

> 日志默认放在  /tmp/gong/hive.log

```
1. 创建日志目录 
$ mkdir logs


2. 备份文件
$ cp $HIVE_HOME/conf/hive-log4j2.properties.template $HIVE_HOME/conf/hive-log4j2.properties


3. 修改 $HIVE_HOME/conf/hive-log4j2.properties 设置目录
property.hive.log.dir = /opt/apache-hive-3.1.2-bin/logs
```

##### 启动

```bash
1. 启动metastore
nohup hive --service metastore > $HIVE_HOME/logs/metastore.log 2>&1 &


2. 启动hiveserver2
nohup hive --service hiveserver2 > $HIVE_HOME/logs/hiveServer2.log2>&1 &
```

##### 验证是否安装成功

```
验证hive 是否成功
hive --help

查看是否使用tez
hive> set hive.execution.engine;
hive.execution.engine=tez
```

##### beeline 客户端

> 可以自动补全命令

```bash
$ beeline -u jdbc:hive2://centos01:10000 -n gong


链接成功显示
Connecting to jdbc:hive2://centos01:10000
Connected to: Apache Hive (version 3.1.2)
Driver: Hive JDBC (version 3.1.2)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 3.1.2 by Apache Hive
0: jdbc:hive2://centos01:10000>
```

## tez ui

> https://blog.csdn.net/sinat_37690778/article/details/80594571
>
> https://www.jianshu.com/p/ed2675c10b94

##### 下载

```
1. 下载 tomcat
官网: https://tomcat.apache.org/download-10.cgi
$ wget https://downloads.apache.org/tomcat/tomcat-10/v10.0.7/bin/apache-tomcat-10.0.7.zip


2. 下载 tez-ui
官网: https://repository.apache.org/content/repositories/releases/org/apache/tez/tez-ui/
$ wget https://repository.apache.org/content/repositories/releases/org/apache/tez/tez-ui/0.10.0/tez-ui-0.10.0.war


3. 在 /opt/apache-tomcat-10.0.7/webapps 下新建tez目录
$ mkdir -p /opt/apache-tomcat-10.0.7/webapps/tez


4. 将 tez-ui 解压到该目录下
$ unzip tez-ui-0.9.2.war
```

##### configs.env

> 编辑 /opt/apache-tomcat-10.0.7/webapps/tez/config/configs.env 文件, 去掉下面两行的注释

```bash
timeline: "http://centos01:8188",
rm: "http://centos01:8088",
```

##### tez-site.xml

> 编辑 $HADOOP_HOME/etc/hadoop/tez-site.xml 文件, 添加如下内容

```xml
<property>
    <name>tez.history.logging.service.class</name>
    <value>org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService</value>
</property>
<property>
    <name>tez.tez-ui.history-url.base</name>
    <value>http://centos01:8880/tez-ui/</value>
</property>
```

##### 启动 (centos01)

```
1. 启动tomcat
# 修改权限
$ chmod +x -R /opt/apache-tomcat-10.0.7/bin
$ /opt/apache-tomcat-10.0.7/bin/startup.sh


访问界面
http://centos01:8080/tez
```

##### 切换 hive 的引擎(临时生效)

```
# hive 的引擎切换为 MapReduce
hive> set hive.execution.engine=mr;

# hive 的引擎切换为 tez
hive> set hive.execution.engine=tez;

# 查看 hive 引擎
hive> set hive.execution.engine;
```

##### 问题1: 启动时报错

```
报错信息:
Exception in thread "main" java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)


原因:
是系统找不到相关jar包或同一类型的jar包有不同版本存在，系统无法决定使用哪一个。


解决办法:
删除hive中低版本的guava包，把hadoop里的复制到hive的lib目录下即可。


$ cd $HIVE_HOME/lib
$ mv guava-19.0.jar guava-19.0.jar.bak
$ cp $HADOOP_HOME/share/hadoop/common/lib/guava-27.0-jre.jar .
```

## 问题

##### 内网和外网 ip

这是 core-site.xml 的配置

```xml
<property>
    <name>fs.defaultFS</name>
    <value>hdfs://centos01:9820</value>
</property>
```

- 这里写 host 或者外网 ip 地址 namenode都无法启动, 原因在于云主机只有一张内网网卡, 填外网地址的话, 从本机访问本机就会连不上

- 如果写 127.0.0.1 可以启动, 但外网无法访问

- 如果写 0.0.0.0 可以启动, 外网可以访问, 但每台机上文件的地址都要改, xsync 同步之后无法直接使用

- 解决办法是 hosts 文件配置的时候, 当前机上的 ip 使用内网ip

- ifconfig 只能看到内网 ip , 外网 ip 无法看到, 因此每台机器上的 hosts 文件都需要修改

##### 





